{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy:\n",
      "U L L L L L L L L L\n",
      "U U U U U U U U U U\n",
      "U U U U U U U U U U\n",
      "U U U U U U U U U U\n",
      "U U U U U U U U U U\n",
      "U U U U U G U U U U\n",
      "U U U U U U U U U U\n",
      "U U U U U U U U U U\n",
      "U U U U U U U U U U\n",
      "U U U U U U U U U U\n"
     ]
    }
   ],
   "source": [
    "#10. Implement the Markov Decision Processes algorithm to find the optimal policy using the given environment actions, transitions, and rewards. Print the optimal policy for each state.\n",
    "import numpy as np\n",
    "\n",
    "# Define the grid size\n",
    "grid_size = 10\n",
    "\n",
    "# Define actions\n",
    "actions = ['up', 'down', 'left', 'right']\n",
    "\n",
    "# Define rewards\n",
    "rewards = np.zeros((grid_size, grid_size))\n",
    "rewards[0, 0] = 1  # Reward for reaching the goal state\n",
    "\n",
    "# Define transition probabilities\n",
    "transition_probs = {\n",
    "    'up': (lambda x, y: (max(x-1, 0), y)),\n",
    "    'down': (lambda x, y: (min(x+1, grid_size-1), y)),\n",
    "    'left': (lambda x, y: (x, max(y-1, 0))),\n",
    "    'right': (lambda x, y: (x, min(y+1, grid_size-1)))\n",
    "}\n",
    "\n",
    "def value_iteration(grid_size, rewards, transition_probs, actions, gamma=0.9, theta=1e-6):\n",
    "    value_table = np.zeros((grid_size, grid_size))\n",
    "    policy = np.zeros((grid_size, grid_size), dtype=int)\n",
    "    \n",
    "    while True:\n",
    "        delta = 0\n",
    "        for x in range(grid_size):\n",
    "            for y in range(grid_size):\n",
    "                if (x, y) == (10, 10):  # Skip the terminal state\n",
    "                    continue\n",
    "                \n",
    "                v = value_table[x, y]\n",
    "                q_values = []\n",
    "                \n",
    "                for i, action in enumerate(actions):\n",
    "                    (next_x, next_y) = transition_probs[action](x, y)\n",
    "                    q_value = rewards[x, y] + gamma * value_table[next_x, next_y]\n",
    "                    q_values.append(q_value)\n",
    "                \n",
    "                value_table[x, y] = max(q_values)\n",
    "                policy[x, y] = np.argmax(q_values)\n",
    "                delta = max(delta, abs(v - value_table[x, y]))\n",
    "        \n",
    "        if delta < theta:\n",
    "            break\n",
    "    \n",
    "    return policy, value_table\n",
    "\n",
    "def print_policy(policy, actions):\n",
    "    policy_arrows = np.full(policy.shape, ' ')\n",
    "    \n",
    "    for x in range(policy.shape[0]):\n",
    "        for y in range(policy.shape[1]):\n",
    "            if (x, y) == (5, 5):\n",
    "                policy_arrows[x, y] = 'G'  # Goal state\n",
    "            else:\n",
    "                policy_arrows[x, y] = actions[policy[x, y]][0].upper()\n",
    "    \n",
    "    for row in policy_arrows:\n",
    "        print(' '.join(row))\n",
    "\n",
    "policy, value_table = value_iteration(grid_size, rewards, transition_probs, actions)\n",
    "print(\"Optimal Policy:\")\n",
    "print_policy(policy, actions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
