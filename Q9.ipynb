{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ebc7928-aed2-4ce4-98a0-d2036836ea9a",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (896748184.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[3], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    (\"LayerNormalization,\", \"MultiHeadAttention,\", \"Conv1D\")\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Embedding, GlobalAveragePooling1D\n",
    "    , LayerNormalization, MultiHeadAttention, Conv1D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.datasets import imdb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Constants\n",
    "MAXLEN = 200  # Maximum sequence length\n",
    "NUM_HEADS = 2  # Number of attention heads (reduced for simplicity)\n",
    "FF_DIM = 128  # Feed-forward dimension in each Transformer block (reduced for simplicity)\n",
    "NUM_TRANSFORMER_BLOCKS = 2  # Number of Transformer blocks (reduced for simplicity)\n",
    "VOCAB_SIZE = 5000  # Vocabulary size\n",
    "EMBED_DIM = 128  # Embedding dimension (reduced for simplicity)\n",
    "# Load IMDB dataset\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=VOCAB_SIZE)\n",
    "df_train = pd.DataFrame({'review': x_train, 'sentiment': y_train})\n",
    "df_test = pd.DataFrame({'review': x_test, 'sentiment': y_test})\n",
    "print(df_train.head)\n",
    "# print(x_train[0],y_train[0])\n",
    "# Get the word index\n",
    "word_index = imdb.get_word_index()\n",
    "# Reserve the first indices for special tokens\n",
    "word_index = {k: (v + 3) for k, v in word_index.items()}\n",
    "word_index[\"<PAD>\"] = 0\n",
    "word_index[\"<START>\"] = 1\n",
    "word_index[\"<UNK>\"] = 2  # unknown\n",
    "word_index[\"<UNUSED>\"] = 3\n",
    "# Reverse the word index to map integers to words\n",
    "reverse_word_index = {value: key for key, value in word_index.items()}\n",
    "# Function to decode reviews back to words\n",
    "def decode_review(encoded_review):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in encoded_review])\n",
    "# Print the first 5 reviews and their labels\n",
    "for i in range(5):\n",
    "    print(f\"Review {i+1}: {decode_review(x_train[i])}\")\n",
    "    print(f\"Label {i+1}: {y_train[i]}\")\n",
    "    print()\n",
    "y_train_df = pd.DataFrame({'label': y_train})\n",
    "x_test_df = pd.DataFrame({'review': x_test})\n",
    "y_test_df = pd.DataFrame({'label': y_test})\n",
    "# Select half of the dataset using iloc\n",
    "train_size = len(x_train_df) // 4\n",
    "test_size = len(x_test_df) // 4\n",
    "x_train_half = x_train_df.iloc[:train_size]\n",
    "y_train_half = y_train_df.iloc[:train_size]\n",
    "x_test_half = x_test_df.iloc[:test_size]\n",
    "y_test_half = y_test_df.iloc[:test_size]\n",
    "# Ensure sequences are padded to the same length\n",
    "x_train_half_padded = pad_sequences(x_train_half['review'].tolist(), maxlen=MAXLEN)\n",
    "x_test_half_padded = pad_sequences(x_test_half['review'].tolist(), maxlen=MAXLEN)\n",
    "# Build Transformer model\n",
    "def build_transformer_model(maxlen, vocab_size, embed_dim, num_heads, ff_dim,\n",
    "num_transformer_blocks):\n",
    "inputs = Input(shape=(maxlen,))\n",
    "embedding_layer = Embedding(input_dim=vocab_size, output_dim=embed_dim)(inputs)\n",
    "# Positional encoding\n",
    "positions = np.arange(maxlen).reshape(-1, 1)\n",
    "positional_encoding = np.zeros((maxlen, embed_dim))\n",
    "positional_encoding[:, 0::2] = np.sin(positions / 10000**(2 * np.arange(embed_dim)[0::2]\n",
    " / embed_dim))\n",
    "positional_encoding[:, 1::2] = np.cos(positions / 10000**(2 * np.arange(embed_dim)[1::2]\n",
    " / embed_dim))\n",
    "x = embedding_layer + positional_encoding\n",
    "# Transformer blocks\n",
    "for _ in range(num_transformer_blocks):\n",
    "# Multi-head self-attention\n",
    "x1 = LayerNormalization()(x)\n",
    "x2 = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim // num_heads)(x1, x1)\n",
    "x = x1 + x2\n",
    "# Feed-forward network\n",
    "x1 = LayerNormalization()(x)\n",
    "x2 = Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x1)\n",
    "x = x1 + x2\n",
    "# Global average pooling and classification\n",
    "x = GlobalAveragePooling1D()(x)\n",
    "outputs = Dense(1, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "return model\n",
    "# Build and compile the model\n",
    "transformer_model = build_transformer_model(MAXLEN, VOCAB_SIZE, EMBED_DIM, NUM_HEADS, FF_DIM\n",
    "    , NUM_TRANSFORMER_BLOCKS)\n",
    "transformer_model.compile(optimizer=Adam(learning_rate=1e-4), loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"])\n",
    "# Print model summary\n",
    "transformer_model.summary()\n",
    "# Train the model\n",
    "transformer_model.fit(np.array(x_train_half_padded), np.array(y_train_half['label']), epochs\n",
    "=2, batch_size=32, validation_data=(np.array(x_test_half_padded), np.array(y_test_half['\n",
    "label'])))\n",
    "# Evaluate the model\n",
    "loss, accuracy = transformer_model.evaluate(np.array(x_test_half_padded), np.array(\n",
    "    y_test_half['label']))\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "def predict_sentiment(review, model, maxlen):\n",
    "# Tokenize and pad the review\n",
    "review_seq = imdb.get_word_index()\n",
    "review_seq = {k:(v+3) for k,v in review_seq.items()}\n",
    "tokenized_review = [review_seq[word] if word in review_seq and review_seq[word] <\n",
    "VOCAB_SIZE else 2 for word in review.split()]\n",
    "padded_review = pad_sequences([tokenized_review], maxlen=maxlen)\n",
    "# Predict sentiment\n",
    "prediction = model.predict(padded_review)[0, 0]\n",
    "sentiment = \"positive\" if prediction >= 0.5 else \"negative\"\n",
    "confidence = prediction if prediction >= 0.5 else 1 - prediction\n",
    "return sentiment, confidence\n",
    "# Example usage of sentiment analysis function\n",
    "new_review = \"This movie good! The acting was bad and the plot was not engaging.\"\n",
    "sentiment, confidence = predict_sentiment(new_review, transformer_model, MAXLEN)\n",
    "print(f\"Review: '{new_review}'\")\n",
    "print(f\"Predicted Sentiment: {sentiment} (Confidence: {confidence * 100:.2f}%)\")                                                                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a80d79-1f37-4fca-814f-587371910d47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
